{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d657e8-2d52-4ace-8001-6c4f2a0620cc",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ee3070-fce3-4648-a95e-5d03b9a923cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path_to_cv_module = r'c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages'\n",
    "sys.path.append(path_to_cv_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68585572-5388-4f3a-9715-6ba82961d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4f2433-5e98-4d8c-90fa-3ebb0f48af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mp4', 'json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = r'deepfake-detection-challenge\\train_sample_videos'\n",
    "types_ = []\n",
    "\n",
    "for file in os.listdir(files):\n",
    "    types = file.split('.')[1]\n",
    "    if types not in types_:\n",
    "        types_.append(types)\n",
    "types_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122b1fe7-678b-45ba-81e3-bc853f9b45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT = False\n",
    "\n",
    "files = r'deepfake-detection-challenge\\train_sample_videos'\n",
    "frames = r'frames'\n",
    "def get_frames(files, frame_path, ext):\n",
    "    count = 0\n",
    "    for f in tqdm(os.listdir(files)):\n",
    "        if 'mp4' in f:\n",
    "            try:\n",
    "                path = os.path.join(files, f)\n",
    "                cap = cv2.VideoCapture(path)\n",
    "                n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "                \n",
    "                count += 1\n",
    "                # print(n_frames, fps)    # 300, 29 for all videos; all videos are ~10s long\n",
    "                \n",
    "                framess = os.path.join(frame_path, f.split('.')[0])\n",
    "                n = 0\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    cv2.imwrite('{}_{}.{}'.format(framess, str(n).zfill(len(str(n_frames))), ext), frame)\n",
    "                    n += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        print(path)\n",
    "        print(count)\n",
    "if EXTRACT:\n",
    "    get_frames(files, frames, 'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef8d836-8d53-4f7b-b143-19eb1e422f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3166ba8-f624-4f28-ad48-ca71278df1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metadata.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json = [file for file in os.listdir(files) if file.endswith('json')][0]\n",
    "json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4709343e-fc30-4871-8e60-420764b0e197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <th>acifjvzvpm.mp4</th>\n",
       "      <th>acqfdwsrhi.mp4</th>\n",
       "      <th>acxnxvbsxk.mp4</th>\n",
       "      <th>acxwigylke.mp4</th>\n",
       "      <th>aczrgyricp.mp4</th>\n",
       "      <th>...</th>\n",
       "      <th>esnntzzajv.mp4</th>\n",
       "      <th>esxrvsgpvb.mp4</th>\n",
       "      <th>esyhwdfnxs.mp4</th>\n",
       "      <th>esyrimvzsa.mp4</th>\n",
       "      <th>etdcqxabww.mp4</th>\n",
       "      <th>etejaapnxh.mp4</th>\n",
       "      <th>etmcruaihe.mp4</th>\n",
       "      <th>etohcvnzbj.mp4</th>\n",
       "      <th>eudeqjhdfd.mp4</th>\n",
       "      <th>eukvucdetx.mp4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "      <td>None</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "      <td>kbvibjhfzo.mp4</td>\n",
       "      <td>ccfoszqabv.mp4</td>\n",
       "      <td>fjlyaizcwc.mp4</td>\n",
       "      <td>ffcwhpnpuw.mp4</td>\n",
       "      <td>slwkmefgde.mp4</td>\n",
       "      <td>...</td>\n",
       "      <td>ybetenmsye.mp4</td>\n",
       "      <td>gomwfvijiv.mp4</td>\n",
       "      <td>qeumxirsme.mp4</td>\n",
       "      <td>qzklcjjxdq.mp4</td>\n",
       "      <td>gipbyjfxfp.mp4</td>\n",
       "      <td>wtreibcmgm.mp4</td>\n",
       "      <td>afoovlsmtx.mp4</td>\n",
       "      <td>bdnaqemxmr.mp4</td>\n",
       "      <td>None</td>\n",
       "      <td>gjypopglvi.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aagfhgtpmv.mp4  aapnvogymq.mp4 abarnvbtwb.mp4  abofeumbvv.mp4  \\\n",
       "label               FAKE            FAKE           REAL            FAKE   \n",
       "split              train           train          train           train   \n",
       "original  vudstovrck.mp4  jdubbvfswz.mp4           None  atvmxvwyns.mp4   \n",
       "\n",
       "          abqwwspghj.mp4  acifjvzvpm.mp4  acqfdwsrhi.mp4  acxnxvbsxk.mp4  \\\n",
       "label               FAKE            FAKE            FAKE            FAKE   \n",
       "split              train           train           train           train   \n",
       "original  qzimuostzz.mp4  kbvibjhfzo.mp4  ccfoszqabv.mp4  fjlyaizcwc.mp4   \n",
       "\n",
       "          acxwigylke.mp4  aczrgyricp.mp4  ...  esnntzzajv.mp4  esxrvsgpvb.mp4  \\\n",
       "label               FAKE            FAKE  ...            FAKE            FAKE   \n",
       "split              train           train  ...           train           train   \n",
       "original  ffcwhpnpuw.mp4  slwkmefgde.mp4  ...  ybetenmsye.mp4  gomwfvijiv.mp4   \n",
       "\n",
       "          esyhwdfnxs.mp4  esyrimvzsa.mp4  etdcqxabww.mp4  etejaapnxh.mp4  \\\n",
       "label               FAKE            FAKE            FAKE            FAKE   \n",
       "split              train           train           train           train   \n",
       "original  qeumxirsme.mp4  qzklcjjxdq.mp4  gipbyjfxfp.mp4  wtreibcmgm.mp4   \n",
       "\n",
       "          etmcruaihe.mp4  etohcvnzbj.mp4 eudeqjhdfd.mp4  eukvucdetx.mp4  \n",
       "label               FAKE            FAKE           REAL            FAKE  \n",
       "split              train           train          train           train  \n",
       "original  afoovlsmtx.mp4  bdnaqemxmr.mp4           None  gjypopglvi.mp4  \n",
       "\n",
       "[3 rows x 400 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('deepfake-detection-challenge/train_sample_videos/metadata.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1d86e4-0c5b-4531-9586-64294c1aa4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b967e6-ae8d-4ba3-9a27-0a71093bc8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af39189-a17f-414e-a4ad-1a1f63693f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 'split', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0236a57-2e57-4664-98a9-200d0659789d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label        original\n",
       "aagfhgtpmv.mp4  FAKE  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL            None\n",
       "abofeumbvv.mp4  FAKE  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  qzimuostzz.mp4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c9df43-ca16-4608-9a8d-3aeb3477cb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6b4d0c-bd3f-481a-8cb1-911db0e513ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "FAKE    323\n",
       "REAL     77\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = data.groupby('label')['label'].count()\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525c28b6-a942-404e-a2cd-9bea4d2f45be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data['label']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969a2419-1757-4729-917f-3202728ffedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7b7f1-4a21-431a-8eba-ef4156585204",
   "metadata": {},
   "source": [
    "# GETTING IMAGES AND LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64e519e-9316-479d-8cbc-75d5d2764257",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for row in data.index:\n",
    "    index_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23736bef-9c37-49d1-b86b-9780a52faaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6210a16f-87a7-49cc-8e7b-5837128138e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aagfhgtpmv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(index_list)):\n",
    "    index_list[i] = index_list[i].split('.')[0]\n",
    "index_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f73fe-e941-48f3-a1c7-f77b2d42079c",
   "metadata": {},
   "source": [
    "images_path = 'frames'\n",
    "images = os.listdir(images_path)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92b45-c4ee-461d-947c-afc242194521",
   "metadata": {},
   "source": [
    "images[0].split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012976d-3825-4e3b-a389-4919b9a1d7a1",
   "metadata": {},
   "source": [
    "data.loc[index_list[0]+'.mp4', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeab30f-7db4-48dd-b7df-5191cd340c6b",
   "metadata": {},
   "source": [
    "data.loc[images[0].split('_')[0]+'.mp4', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40a9a4-f7bf-4140-ab09-a2a20a9d631c",
   "metadata": {},
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6786a3a-58e2-4cae-b9ff-08f5ad4d3887",
   "metadata": {},
   "source": [
    "index_list[1], index_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe24b1-fcce-4e67-a1fb-f38d5e189ea4",
   "metadata": {},
   "source": [
    "images[300], images[600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5e91a-aad6-4f3f-bc7a-4555f6918afe",
   "metadata": {},
   "source": [
    "# TRAIN, VALIDATION, TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b634bc-0af3-4135-9bd5-fcfe385d5aaf",
   "metadata": {},
   "source": [
    "from torch.utils.data import random_split\n",
    "MANUAL_SEED = torch.Generator().manual_seed(42)\n",
    "train, test = random_split(images, [71984, 47990], generator = MANUAL_SEED)    # 60, 40 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850092a-c647-4b72-9e65-f4e644417cb3",
   "metadata": {},
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503839a-5c7a-4ceb-b26f-098bac373b1b",
   "metadata": {},
   "source": [
    "validation, test = random_split(test, [23995, 23995])   #50, 50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531ad12-f151-4ebb-8c0b-d8eefb1751fa",
   "metadata": {},
   "source": [
    "len(validation), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae19b1-35c8-46d1-8920-02257d147a09",
   "metadata": {},
   "source": [
    "train[0], test[0], validation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a23cc-1f90-4734-bac3-ccd0fb011a0b",
   "metadata": {},
   "source": [
    "os.path.join('frames', train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d4d3e-d276-4796-97ea-56e00fd1606c",
   "metadata": {},
   "source": [
    "data.loc[train[0].split('_')[0]+'.mp4']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d39e82-a373-47a0-80e4-9afa5aa3d82d",
   "metadata": {},
   "source": [
    "#img = cv2.imread(os.path.join('frames', train[0]), cv2.IMREAD_COLOR)\n",
    "#cv2.imshow('color image', img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "img = plt.imread(os.path.join('frames', train[0]))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d592af6-5f9c-4a06-8c09-4edb37a84dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = plt.imread(os.path.join('frames', images[0]))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffdb087-c9cc-4213-abcf-0cafc77bbd76",
   "metadata": {},
   "source": [
    "### MAKING REAL FAKE TRAIN TEST VALIDATION FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4b91a30-35a1-47b6-b2f7-05c8d89f82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "MOVE = False\n",
    "def move_images(frames_directory, dataset_directory, split_info):\n",
    "    if not os.path.exists(frames_directory):\n",
    "        print(f\"Frames directory '{frames_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    for category in split_info:\n",
    "        for label in split_info[category]:\n",
    "            directory = os.path.join(dataset_directory, category, label)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "    for root, _, files in os.walk(frames_directory):\n",
    "        for file in files:\n",
    "            if file in test:\n",
    "                split = 'test'\n",
    "            elif file in train:\n",
    "                split = 'train'\n",
    "            else:\n",
    "                split = 'validation'\n",
    "            label = 'REAL' if data.loc[file.split('_')[0]+'.mp4']['label'] == 'REAL' else 'FAKE'\n",
    "            source_path = os.path.join(root, file)\n",
    "            destination_directory = os.path.join(dataset_directory, split, label)\n",
    "            destination_path = os.path.join(destination_directory, file)\n",
    "\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"Moved '{file}' to '{destination_path}'\")\n",
    "\n",
    "split_info = {\n",
    "    'train': ['REAL', 'FAKE'],\n",
    "    'test': ['REAL', 'FAKE'],\n",
    "    'validation': ['REAL', 'FAKE']\n",
    "}\n",
    "\n",
    "if MOVE:\n",
    "    move_images('frames', 'Dataset', split_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f34995-acf1-4737-92f0-42950b121086",
   "metadata": {},
   "source": [
    "# DATA TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c6e4fdc-8007-46eb-931e-917ce6357118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0678167-f9d1-4227-b8a0-ad1afb32188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae860731-0b97-4d7a-bd64-7941533cc0b5",
   "metadata": {},
   "source": [
    "image_path = 'Dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567701a-76aa-4ee0-b2c5-b6ed2f288a7a",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*.jpg\"))\n",
    "plot_transformed_images(image_path_list, \n",
    "                        transform=data_transform, \n",
    "                        n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e114207-1f44-487f-88ff-ce81e11c9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/Train'\n",
    "test_dir = 'Dataset/Test'\n",
    "validation_dir = 'Dataset/Validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a6c2a22-70b4-42f4-af2f-d83ad10270c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(root = train_dir, # target folder of images\n",
    "                                  transform = data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform = None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root = test_dir, \n",
    "                                 transform = data_transform)\n",
    "\n",
    "validation_data = datasets.ImageFolder(root = validation_dir, \n",
    "                                 transform = data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "debc5dc3-de6c-486f-b318-28e492ca648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAKE', 'REAL']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb69ebe0-d208-430a-994c-351805d0e71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAKE': 0, 'REAL': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb4d8ee-a57d-47c0-940b-ec998cc7cb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71984, 23995, 23995)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fb89f3c-994e-407c-bd32-c107064b1a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor:\n",
      "tensor([[[0.7882, 0.7882, 0.7882,  ..., 0.8039, 0.7961, 0.7961],\n",
      "         [0.7922, 0.7922, 0.7882,  ..., 0.8078, 0.7961, 0.7961],\n",
      "         [0.7961, 0.7961, 0.7882,  ..., 0.8118, 0.8000, 0.7961],\n",
      "         ...,\n",
      "         [0.7569, 0.7647, 0.8039,  ..., 0.8000, 0.8157, 0.8392],\n",
      "         [0.7569, 0.7608, 0.8078,  ..., 0.7961, 0.8118, 0.8353],\n",
      "         [0.7529, 0.7608, 0.8078,  ..., 0.7922, 0.8078, 0.8314]],\n",
      "\n",
      "        [[0.8314, 0.8314, 0.8314,  ..., 0.8235, 0.8275, 0.8314],\n",
      "         [0.8314, 0.8314, 0.8314,  ..., 0.8235, 0.8275, 0.8314],\n",
      "         [0.8314, 0.8314, 0.8314,  ..., 0.8235, 0.8275, 0.8314],\n",
      "         ...,\n",
      "         [0.8353, 0.8392, 0.8824,  ..., 0.8196, 0.8588, 0.9020],\n",
      "         [0.8353, 0.8392, 0.8863,  ..., 0.8235, 0.8588, 0.9020],\n",
      "         [0.8392, 0.8431, 0.8863,  ..., 0.8235, 0.8588, 0.9059]],\n",
      "\n",
      "        [[0.7686, 0.7686, 0.7686,  ..., 0.7373, 0.7333, 0.7255],\n",
      "         [0.7686, 0.7686, 0.7686,  ..., 0.7373, 0.7333, 0.7255],\n",
      "         [0.7647, 0.7647, 0.7686,  ..., 0.7412, 0.7373, 0.7294],\n",
      "         ...,\n",
      "         [0.8039, 0.8314, 0.9333,  ..., 0.7647, 0.8431, 0.9216],\n",
      "         [0.8196, 0.8431, 0.9412,  ..., 0.7686, 0.8471, 0.9255],\n",
      "         [0.8275, 0.8471, 0.9451,  ..., 0.7686, 0.8471, 0.9255]]])\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Image datatype: torch.float32\n",
      "Image label: 0\n",
      "Label datatype: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07bec4d9-212f-4f49-a38f-f515dc2843a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1bf81896af0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1bf818968b0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1bf8188bbb0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset = train_data, \n",
    "                              batch_size = 32, # how many samples per batch?\n",
    "                              num_workers = os.cpu_count(), # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle = True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_data, \n",
    "                             batch_size = 32, \n",
    "                             num_workers = os.cpu_count(), \n",
    "                             shuffle = False) # don't usually need to shuffle testing data\n",
    "\n",
    "validation_dataloader = DataLoader(dataset = validation_data, \n",
    "                             batch_size = 32, \n",
    "                             num_workers = os.cpu_count(), \n",
    "                             shuffle = False)\n",
    "train_dataloader, test_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134e6a7-fd2e-48dd-9d04-b7a6de592ecd",
   "metadata": {},
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d27eb-7466-4355-b9d8-34f42cdbd0e7",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042d782-81ed-4e6a-bc4f-843cbb7eeb5c",
   "metadata": {},
   "source": [
    "path = os.walk('frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83e9f2-0906-4c83-b704-32abe723cae6",
   "metadata": {},
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba6fa1-091b-4244-9a6f-e9a332a645ba",
   "metadata": {},
   "source": [
    "list(path)[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da37b504-db10-4c2a-ba9a-7dabc1eee41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=28090, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*53*53,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fc84aae-7489-4102-9695-b93444c89845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image shape: torch.Size([1, 3, 224, 224])\n",
      "\n",
      "Output logits:\n",
      "tensor([[0.0326, 0.0052]], device='cuda:0')\n",
      "\n",
      "Output prediction probabilities:\n",
      "tensor([[0.5068, 0.4932]], device='cuda:0')\n",
      "\n",
      "Output prediction label:\n",
      "tensor([0], device='cuda:0')\n",
      "\n",
      "Actual label:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 1. Get a batch of images and labels from the DataLoader\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Perform a forward pass on a single image\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_0(img_single.to(device))\n",
    "    \n",
    "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cecf310-decc-47c0-b6df-45ae25bb93b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [1, 2]                    --\n",
       "├─Sequential: 1-1                        [1, 10, 110, 110]         --\n",
       "│    └─Conv2d: 2-1                       [1, 10, 222, 222]         280\n",
       "│    └─ReLU: 2-2                         [1, 10, 222, 222]         --\n",
       "│    └─Conv2d: 2-3                       [1, 10, 220, 220]         910\n",
       "│    └─ReLU: 2-4                         [1, 10, 220, 220]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 10, 110, 110]         --\n",
       "├─Sequential: 1-2                        [1, 10, 53, 53]           --\n",
       "│    └─Conv2d: 2-6                       [1, 10, 108, 108]         910\n",
       "│    └─ReLU: 2-7                         [1, 10, 108, 108]         --\n",
       "│    └─Conv2d: 2-8                       [1, 10, 106, 106]         910\n",
       "│    └─ReLU: 2-9                         [1, 10, 106, 106]         --\n",
       "│    └─MaxPool2d: 2-10                   [1, 10, 53, 53]           --\n",
       "├─Sequential: 1-3                        [1, 2]                    --\n",
       "│    └─Flatten: 2-11                     [1, 28090]                --\n",
       "│    └─Linear: 2-12                      [1, 2]                    56,182\n",
       "==========================================================================================\n",
       "Total params: 59,192\n",
       "Trainable params: 59,192\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 78.74\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 9.65\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 10.49\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 3, 224, 224]) # do a test pass through of an example input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3c9a900-f7d4-476b-bbcd-832352c5d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0388ce3-1546-45ae-94f2-62a1dc3bebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "085604c6-80b5-4916-b5b1-e48cbd92b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4268df91-98d8-4d1b-a464-bab8255b0fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6263f5ce76e74e4282c120fa5a271c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.3830 | train_acc: 0.8265 | test_loss: 0.3378 | test_acc: 0.8357\n",
      "Epoch: 2 | train_loss: 0.2995 | train_acc: 0.8610 | test_loss: 0.2844 | test_acc: 0.8684\n",
      "Epoch: 3 | train_loss: 0.2652 | train_acc: 0.8731 | test_loss: 0.2850 | test_acc: 0.8721\n",
      "Epoch: 4 | train_loss: 0.2449 | train_acc: 0.8838 | test_loss: 0.2721 | test_acc: 0.8785\n",
      "Epoch: 5 | train_loss: 0.2300 | train_acc: 0.8902 | test_loss: 0.2468 | test_acc: 0.8843\n",
      "Total training time: 28194.582 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "334fcab8-aa54-4bd1-8ff1-dc7f1261467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\deepfake_model_0.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "MODEL_PATH = Path('models')\n",
    "MODEL_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME = 'deepfake_model_0.pth'\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "\n",
    "# Save the model state dict\n",
    "print(f'Saving model to: {MODEL_SAVE_PATH}')\n",
    "torch.save(obj = model_0.state_dict(), f = MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6203aef-836d-41bb-8b58-41cf59c9435e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
